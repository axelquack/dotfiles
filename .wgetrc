# Use the server-provided last modification date, if available
timestamping = on

# Do not go up in the directory structure when downloading recursively
## no_parent = on

# Wait 60 seconds before timing out. This applies to all timeouts: DNS, connect and read. (The default read timeout is 15 minutes!)
timeout = 60

# Retry a few times when a download fails, but donâ€™t overdo it. (The default is 20!)
tries = 3

# Retry even when the connection was refused
retry_connrefused = on

# Use the last component of a redirection URL for the local file name
trust_server_names = on

# Follow FTP links from HTML documents by default
follow_ftp = off

# Add a `.html` extension to `text/html` or `application/xhtml+xml` files that lack one, or a `.css` extension to `text/css` files that lack one
adjust_extension = on

# Ignore `robots.txt` and `<meta name=robots content=nofollow>`
robots = off

# Print the HTTP and FTP server responses
server_response = on

# Define a comma-separated list of file types that wget will not download
# Useful when you want to exclude certain file types
reject = exe,zip

# Set the maximum depth of recursion when downloading a website recursively
# Replace 'N' with the desired recursion depth (e.g., 5, 10, etc.)
## reclevel = N

# Define a directory prefix where the downloaded files will be saved
# Replace '/path/to/your/directory' with the desired directory path
## dir_prefix = /path/to/your/directory

# Set the number of concurrent connections to be used when downloading recursively
# Replace 'N' with the desired number of concurrent connections (e.g., 5, 10, etc.)
## concurrent = N
